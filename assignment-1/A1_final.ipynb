{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_noise = pd.read_csv(\"data/zero_noise.csv\")\n",
    "low_noise = pd.read_csv(\"data/low_noise.csv\")\n",
    "high_noise = pd.read_csv(\"data/high_noise.csv\")\n",
    "low_noise.drop(columns=[\"data_type\"], inplace=True)\n",
    "high_noise.drop(columns=[\"data_type\"], inplace=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(v, class_values):\n",
    "    return class_values.index(v)\n",
    "\n",
    "def encode_target(v, class_values):\n",
    "    return class_values.index(v)\n",
    "\n",
    "class_values_zero = list(zero_noise.era.unique())\n",
    "zero_noise[\"era_label\"] = zero_noise[\"era\"].apply(encode, args=(class_values_zero,))\n",
    "\n",
    "class_values_low = list(low_noise.era.unique())\n",
    "class_values_low_target_5 = list(low_noise.target_5_val.unique())\n",
    "class_values_low_target_10 = list(low_noise.target_10_val.unique())\n",
    "low_noise[\"era_label\"] = low_noise[\"era\"].apply(encode, args=(class_values_low,))\n",
    "low_noise[\"target_5_val_label\"] = low_noise[\"target_5_val\"].apply(encode_target, args=(class_values_low_target_5,))\n",
    "low_noise[\"target_10_val_label\"] = low_noise[\"target_10_val\"].apply(encode_target, args=(class_values_low_target_10,))\n",
    "\n",
    "class_values_high = list(high_noise.era.unique())\n",
    "class_values_high_target_5 = list(high_noise.target_5_val.unique())\n",
    "class_values_high_target_10 = list(high_noise.target_10_val.unique())\n",
    "high_noise[\"era_label\"] = high_noise[\"era\"].apply(encode, args=(class_values_high,))\n",
    "high_noise[\"target_5_val_label\"] = high_noise[\"target_5_val\"].apply(encode_target, args=(class_values_high_target_5,))\n",
    "high_noise[\"target_10_val_label\"] = high_noise[\"target_10_val\"].apply(encode_target, args=(class_values_high_target_10,))\n",
    "\n",
    "dataset = high_noise\n",
    "target_column = \"target_10_val_label\"\n",
    "output_classes = 12 if target_column==\"era_label\" else 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.X[index], dtype=torch.float32, device=device),\n",
    "            torch.tensor(self.y[index], dtype=torch.long, device=device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "train_dataset = dataset.iloc[:train_size]\n",
    "val_dataset = dataset.iloc[train_size:train_size+val_size]\n",
    "test_dataset = dataset.iloc[train_size+val_size:]\n",
    "\n",
    "train_X = train_dataset.iloc[:, :-8].values\n",
    "train_y = train_dataset.loc[:, target_column].values\n",
    "\n",
    "val_X = val_dataset.iloc[:, :-8].values\n",
    "val_y = val_dataset.loc[:, target_column].values\n",
    "\n",
    "test_X = test_dataset.iloc[:, :-8].values\n",
    "test_y = test_dataset.loc[:, target_column].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_X, train_y)\n",
    "val_dataset = CustomDataset(val_X, val_y)\n",
    "test_dataset = CustomDataset(test_X, test_y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.linear = nn.Linear(output_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits = self.layers[-1](x).to(torch.float64).to(device)\n",
    "        x1 = self.linear(x)\n",
    "        tau = torch.sigmoid(x1).to(torch.float64).to(device)\n",
    "\n",
    "        return logits, tau\n",
    "    \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=24, encoding_dim=16):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, encoding_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class NoiseAttentionLoss(nn.Module):\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super(NoiseAttentionLoss, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, logits, y, tau):\n",
    "        y = F.one_hot(y, output_classes).to(torch.float64).to(device)\n",
    "        perceptual = tau.squeeze(1) * (logits.t() - y.t()) + y.t()\n",
    "        attention_term = torch.matmul(y, torch.log(perceptual + 1e-8))\n",
    "        attention_term = attention_term.diag()\n",
    "        boost_term = torch.log(tau + 1e-8) * self.lambda_\n",
    "\n",
    "        attention_term = -torch.mean(attention_term)\n",
    "        boost_term = -torch.mean(boost_term)\n",
    "\n",
    "        return attention_term + boost_term\n",
    "\n",
    "\n",
    "class SubTab(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubTab, self).__init__\n",
    "        \n",
    "\n",
    "layers = nn.ModuleList(\n",
    "    [\n",
    "        nn.Linear(24, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, output_classes),\n",
    "        nn.Softmax(dim=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "autoencoder = Autoencoder(input_dim=24, encoding_dim=16).to(device)\n",
    "model = MLP(layers).to(device)\n",
    "\n",
    "learning_rate = 0.0025\n",
    "learning_rate_auto = 0.001\n",
    "weight_decay = 0.0001\n",
    "epochs = 50\n",
    "epochs_auto = 50\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = NoiseAttentionLoss(lambda_=50).to(device)\n",
    "criterion_auto = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_auto = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataloader, epochs, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for features, _ in iter(dataloader):\n",
    "            input = features.to(device)\n",
    "            target = features.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# train_autoencoder(autoencoder, train_dataloader, epochs_auto, criterion_auto, optimizer_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, epochs, train_dataloader, val_dataloader,\n",
    "          bagging_mode=False, validation_mode=True, NAL_loss=False, debug_mode=False):\n",
    "    \n",
    "    batch=0\n",
    "    val_losses=[]\n",
    "    val_accuracies=[]\n",
    "    train_losses=[]\n",
    "    train_accuracies=[]\n",
    "    num_batches=int(len(train_dataloader)*0.9)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        train_accuracy=0.0\n",
    "        train_loss=0.0\n",
    "        model.train()\n",
    "        batch=0\n",
    "        for feature ,label in iter(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            if NAL_loss:\n",
    "                output, tau = model(feature)\n",
    "                loss = loss_fn(output, label, tau)\n",
    "            else:\n",
    "                output, tau = model(feature)\n",
    "                loss = loss_fn(output, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss+=(loss.item())\n",
    "            train_accuracy+=torch.sum(torch.argmax(output, dim=1) == label)/len(label)\n",
    "\n",
    "            batch+=1\n",
    "            if bagging_mode:\n",
    "                if batch==num_batches:\n",
    "                    break\n",
    "            if debug_mode:\n",
    "                break\n",
    "            \n",
    "        train_losses.append(train_loss/len(train_dataloader))\n",
    "        train_accuracies.append(train_accuracy/len(train_dataloader))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training loss: {train_loss}\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training accuracy: {train_accuracy/len(train_dataloader)}\")\n",
    "        print(\"-----------------------------------------------\")\n",
    "        \n",
    "        if validation_mode:\n",
    "            val_loss, val_acc = validation(model, val_dataloader, loss_fn)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "        batch=0\n",
    "\n",
    "    if validation_mode:\n",
    "        return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "    else:\n",
    "        return train_losses, train_accuracies\n",
    "\n",
    "def validation(model, val_dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for feature, label in iter(val_dataloader):\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(feature)\n",
    "            val_loss += criterion(output, label).item()\n",
    "            val_accuracy += torch.sum(torch.argmax(output, dim=1) == label)/len(label)\n",
    "            \n",
    "        print(f\"Validation loss: {val_loss/len(val_dataloader)}\")\n",
    "        print(f\"Validation accuracy: {val_accuracy/len(val_dataloader)}\")\n",
    "\n",
    "    return val_loss/len(val_dataloader), val_accuracy/len(val_dataloader)\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "        for feature, label in iter(test_dataloader):\n",
    "            feature, label = feature.to(device), label.to(device)\n",
    "            outputs, _ = model(feature)\n",
    "            accuracy += torch.sum(torch.argmax(outputs, dim=1) == label)/len(label)\n",
    "        print(f\"Test accuracy: {accuracy/len(test_dataloader)}\")\n",
    "\n",
    "def plot(train, val, mode):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train, label=f\"Training {mode}\")\n",
    "    plt.plot(val, label=f\"Validation {mode}\")\n",
    "    plt.title(f\"Training and Validation {mode} Curve\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(f\"{mode}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_train(loss_fn, epochs, train_dataloader, val_dataloader):\n",
    "\n",
    "    models=[]\n",
    "    optims=[]\n",
    "    schedulers=[]\n",
    "    train_losses=[]\n",
    "    train_accuracies=[]\n",
    "    val_losses=[]\n",
    "    val_accuracies=[]\n",
    "    for i in range(5):\n",
    "        model = MLP(layers).to(device)\n",
    "        models.append(model)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.003,weight_decay=0.0001)\n",
    "        optims.append(optimizer)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.1, \n",
    "                                                               threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "        schedulers.append(scheduler)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss=[]\n",
    "        acc=[]\n",
    "        for i in range (len(models)):\n",
    "            model=models[i]\n",
    "            optim=optims[i]\n",
    "            print(f\"Training model {i+1}\")\n",
    "            # print('***********************************************')\n",
    "            train_loss, train_acc=train(model,optim,loss_fn,1,train_dataloader,\n",
    "                                       val_dataloader,bagging_mode=False,validation_mode=False,\n",
    "                                       NAL_loss=True,debug_mode=False)\n",
    "            \n",
    "            loss.append(train_loss); acc.append(train_acc)\n",
    "        \n",
    "        train_losses.append(sum(loss)/len(loss))\n",
    "        train_accuracies.append(sum(acc)/len(acc))\n",
    "\n",
    "        predictions=[]\n",
    "        labels=[]\n",
    "        losses = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            accuracy = 0\n",
    "            running_loss = 0\n",
    "            model_predictions=[]\n",
    "            model_labels=[]\n",
    "            with torch.no_grad():\n",
    "                for feature, label in iter(val_dataloader):\n",
    "                    feature = feature.to(device)\n",
    "                    label = label.to(device)\n",
    "                    output, tau = model(feature)\n",
    "                    prediction = torch.argmax(output, dim=1)\n",
    "                    model_predictions.extend(prediction.item() for prediction in prediction)\n",
    "                    model_labels.extend(label.item() for label in label)\n",
    "                    loss = loss_fn(output, label, tau)\n",
    "                    running_loss += loss.item()\n",
    "            losses.append(running_loss)\n",
    "            predictions.append(model_predictions)\n",
    "            labels.append(model_labels)\n",
    "\n",
    "        val_loss = sum(losses)/len(losses)\n",
    "        final_labels=labels[0]\n",
    "        final_predictions=[]\n",
    "        for i in range(len(predictions[0])):\n",
    "            current_predictions=[prediction[i] for prediction in predictions]\n",
    "            final_predictions.append(max(set(current_predictions), key=current_predictions.count))\n",
    "        accuracy = sum(1 for x,y in zip(final_predictions, final_labels) if x==y)/len(final_predictions)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(\"Validation\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Validation loss: {val_loss}\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Validation accuracy: {accuracy}\")\n",
    "        print(\"===============================================\")\n",
    "\n",
    "    return models, train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6673f109cb694510bd5580d0e7fb2400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m NoiseAttentionLoss(lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m bagging_models, train_losses, train_accuracies, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mbagging_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 26\u001b[0m, in \u001b[0;36mbagging_train\u001b[1;34m(loss_fn, epochs, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# print('***********************************************')\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     train_loss, train_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbagging_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mvalidation_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mNAL_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdebug_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     loss\u001b[38;5;241m.\u001b[39mappend(train_loss); acc\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[0;32m     32\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(loss))\n",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, epochs, train_dataloader, val_dataloader, bagging_mode, validation_mode, NAL_loss, debug_mode)\u001b[0m\n\u001b[0;32m     25\u001b[0m     output, tau \u001b[38;5;241m=\u001b[39m model(feature)\n\u001b[0;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output, label)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\Jaskaran\\miniconda3\\envs\\mlenv\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jaskaran\\miniconda3\\envs\\mlenv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = NoiseAttentionLoss(lambda_=50).to(device)\n",
    "bagging_models, train_losses, train_accuracies, val_losses, val_accuracies = bagging_train(criterion, epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
